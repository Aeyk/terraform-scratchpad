#!/usr/bin/env bash

set -o xtrace
set -o errexit
set -o errtrace
set -o pipefail

export DEBIAN_FRONTEND=noninteractive
export NEEDRESTART_MODE=a
sudo apt-get remove needrestart --yes


# ## Generate and add ssh key to authorized users
# chmod 700 ~/.ssh
# touch ~/.ssh/authorized_keys
# chmod 600 ~/.ssh/authorized_keys
# test -e "$HOME"/.ssh/id_rsa || ssh-keygen -f "$HOME"/.ssh/id_rsa -P ""
# cat "$HOME"/.ssh/id_rsa.pub >> "$HOME"/.ssh/authorized_keys

## Install ansible
sudo apt update
# sudo apt install software-properties-common --yes
# sudo add-apt-repository --yes --update ppa:ansible/ansible
sudo apt install --yes python3.10-full
# sudo apt purge --yes python3
curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py
python3.10 get-pip.py
$HOME/.local/bin/pip install ruamel_yaml netaddr jmespath==0.9.5 ansible==7.6.0
# sudo mkdir -p /data/{elasticsearch,kibana,archivebox,gitea}

echo 'export PATH=$${PATH:+$${PATH}:}$HOME/.local/bin/' >> "$HOME"/.bashrc
export PATH=$${PATH:+$${PATH}:}$HOME/.local/bin/

## Install kubespray
cd /tmp || exit
git clone https://github.com/kubernetes-sigs/kubespray || true
cd kubespray || exit
git checkout release-2.23

# Copy ``inventory/sample`` as ``inventory/mycluster``
cp -rfp inventory/sample inventory/main

# Update Ansible inventory file with inventory builder

if test 0 = "$${#IPS[@]}"; then
    # If we pass in IPS array (indicating vagrant) don't set to cloud ips
    declare -a IPS=(
        %{ for ip in slice(arm-1vcpu-6gb-us-qas-private_ipv4, 1, 4) ~}
        ${ ip }
        %{ endfor ~}
    )
%{ for index, ip in slice(arm-1vcpu-6gb-us-qas-private_ipv4, 1, 4) ~}
    if ! grep -qxF -- "node${index} ${ ip }" /etc/hosts; then 
        echo "node${index} ${ ip }" | sudo tee -a /etc/hosts
    fi
%{ endfor ~}
fi

KUBE_MASTERS=3 KUBE_CONTROL_HOST=3 CONFIG_FILE=inventory/main/hosts.yaml python3 contrib/inventory_builder/inventory.py "$${IPS[@]}"

# Review and change parameters under ``inventory/main/group_vars``
# cat inventory/main/group_vars/all/all.yml
# cat inventory/main/group_vars/k8s_cluster/k8s-cluster.yml

sed -i 's/metrics_server_enabled: false/metrics_server_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
sed -i 's/ingress_nginx_enabled: false/ingress_nginx_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
sed -i 's/cert_manager_enabled: false/cert_manager_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
sed -i 's/^# kubeconfig_localhost: false/kubeconfig_localhost: true/g' inventory/main/group_vars/k8s_cluster/k8s-cluster.yml
sed -i 's/^# kubectl_localhost: false/kubectl_localhost: true/g' inventory/main/group_vars/k8s_cluster/k8s-cluster.yml
# sed -i 's/helm_enabled: false/helm_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml

# sed -i 's/kube_proxy_strict_arp: false/kube_proxy_strict_arp: true/g' inventory/main/group_vars/k8s_cluster/addons.ymli
yaml=""
for bgp_ip in "$${IPS[@]}"; do
    yaml+="- $bgp_ip:65000::false
"
done
echo "
kube_vip_enabled: true
kube_vip_controlplane_enabled: true
kube_vip_address: 10.0.0.254
loadbalancer_apiserver:
  address: \"{{ kube_vip_address }}\"
  port: 6443
kube_vip_services_enabled: true

kube_vip_bgp_enabled: true
kube_vip_local_as: 65000
kube_vip_bgp_routerid: 10.0.0.1
kube_vip_bgppeers:
$yaml
" >> inventory/main/group_vars/k8s_cluster/addons.yml

# kubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml
# kubectl create configmap -n kube-system kubevip --from-literal cidr-global=10.69.0.0/16

# sed -i 's/metallb_speaker_enabled: false/metallb_speaker_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
# sed -i 's/kube_proxy_strict_arp: false/kube_proxy_strict_arp: true/g' inventory/main/group_vars/k8s_cluster/k8s-cluster.yml
# echo 'metallb_config:
#   controller:
#     nodeselector:
#       kubernetes.io/os: linux
#     tolerations:
#     - key: "node-role.kubernetes.io/control-plane"
#       operator: "Equal"
#       value: ""
#       effect: "NoSchedule"
#   address_pools:
#     primary:
#       ip_range:
# %{ for ip in slice(arm-1vcpu-6gb-us-qas-public_ipv4, 1, 4) ~}
#         - ${ ip }
# %{ endfor ~}
#   layer2:
#     - primary' >> inventory/main/group_vars/k8s_cluster/addons.yml

# TODO CA certificate

# Clean up old Kubernetes cluster with Ansible Playbook - run the playbook as root
# The option `--become` is required, as for example cleaning up SSL keys in /etc/,
# uninstalling old packages and interacting with various systemd daemons.
# Without --become the playbook will fail to run!
# And be mind it will remove the current kubernetes cluster (if it's running)!
ansible-playbook -i inventory/main/hosts.yaml  --become --become-user=root \
                 --extra-vars reset_confirmation=true --extra-vars log_path=/tmp/kubespray-reset.log \
                 reset.yml

# Deploy Kubespray with Ansible Playbook - run the playbook as root
# The option `--become` is required, as for example writing SSL keys in /etc/,
# installing packages and interacting with various systemd daemons.
# Without --become the playbook will fail to run!
ansible-playbook -i inventory/main/hosts.yaml  --become --become-user=root \
                 -e kube_network_plugin=cilium --extra-vars log_path=/tmp/kubespray-cluster.log \
                 cluster.yml

mkdir -p "$HOME"/.kube
sudo cp -fv /tmp/kubespray/inventory/main/artifacts/admin.conf "$HOME"/.kube/config
# sudo cp -fv /etc/kubernetes/admin.conf "$HOME"/.kube/config
sudo chown "$(id -u):$(id -g)" "$HOME"/.kube/config
