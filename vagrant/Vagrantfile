require "yaml"
settings = YAML.load_file "settings.yaml"

IP_SECTIONS = settings["network"]["control_ip"].match(/^([0-9.]+\.)([^.]+)$/)
# First 3 octets including the trailing dot:
IP_NW = IP_SECTIONS.captures[0]
# Last octet excluding all dots:
IP_START = Integer(IP_SECTIONS.captures[1])
NUM_WORKER_NODES = settings["nodes"]["workers"]["count"]

Vagrant.configure("2") do |config|
  config.vm.provision "shell", env: { "IP_NW" => IP_NW, "IP_START" => IP_START, "NUM_WORKER_NODES" => NUM_WORKER_NODES }, inline: <<-SHELL
      set -euxo pipefail
      # apt-get update -y
      echo "$IP_NW$((IP_START)) master-node" >> /etc/hosts
      for i in `seq 1 ${NUM_WORKER_NODES}`; do
        echo "$IP_NW$((IP_START+i)) worker-node0${i}" >> /etc/hosts
      done
  SHELL
  config.vm.provision "shell", env: { "IP_NW" => IP_NW, "IP_START" => IP_START, "NUM_WORKER_NODES" => NUM_WORKER_NODES }, privileged: false, 
    inline: <<-SHELL
      set -euxo pipefail
      # check for private key for vm-vm comm
      [ -f /vagrant/id_rsa ] || {
        ssh-keygen -t rsa -f /vagrant/id_rsa -q -N ''
      }
      # deploy key
      [ -f /home/vagrant/.ssh/id_rsa ] || {
        cp /vagrant/id_rsa /home/vagrant/.ssh/id_rsa
        chmod 0600 /home/vagrant/.ssh/id_rsa
      }
      # allow ssh passwordless
      ssh vagrant@"$IP_NW$((IP_START))" ~/.ssh/authorized_keys &>/dev/null || {
        cat /vagrant/id_rsa.pub >> ~/.ssh/authorized_keys
        chmod 0600 ~/.ssh/authorized_keys
      }
      for i in `seq 1 ${NUM_WORKER_NODES}`; do
        grep "vagrant@worker-node0${i}" ~/.ssh/authorized_keys &>/dev/null || {
          cat /vagrant/id_rsa.pub >> ~/.ssh/authorized_keys
          chmod 0600 ~/.ssh/authorized_keys
        }
      done
      # exclude node* from host checking
      cat <<-EOF > ~/.ssh/config 
        Host 10.* 
        StrictHostKeyChecking no
        UserKnownHostsFile=/dev/null
        Host worker-node*
        StrictHostKeyChecking no
        UserKnownHostsFile=/dev/null
        Host master-node
        StrictHostKeyChecking no
        UserKnownHostsFile=/dev/null
EOF
SHELL
  config.vm.box = settings["software"]["box"]
  config.vm.box_check_update = true
  config.vm.define "master" do |master|
    master.vm.hostname = "master-node"
    config.vm.box = settings["software"]["box"]
    master.vm.network "private_network", ip: settings["network"]["control_ip"]
    if settings["shared_folders"]
      settings["shared_folders"].each do |shared_folder|
        master.vm.synced_folder shared_folder["host_path"], shared_folder["vm_path"]
      end
    end
    master.vm.provider "virtualbox" do |vb|
      vb.cpus = settings["nodes"]["control"]["cpu"]
      vb.memory = settings["nodes"]["control"]["memory"]
      if settings["cluster_name"] and settings["cluster_name"] != ""
        vb.customize ["modifyvm", :id, "--groups", ("/" + settings["cluster_name"])]
      end
    end
    master.vm.provision "shell", env: { "IP_NW" => IP_NW, "IP_START" => IP_START, "NUM_WORKER_NODES" => NUM_WORKER_NODES }, privileged: false, 
       inline: <<-SHELL
         set +euxo pipefail
         declare -a IPS=()
         echo "$IP_NW$((IP_START)) master-node" >> /etc/hosts
         for i in `seq 1 ${NUM_WORKER_NODES}`; do
           echo "$IP_NW$((IP_START+i)) worker-node0${i}" >> /etc/hostsa
           IPS+=($IP_NW$((IP_START+i))) 
         done
         ## Install ansible
         sudo apt update
         # sudo apt install software-properties-common --yes
         # sudo add-apt-repository --yes --update ppa:ansible/ansible
         sudo apt install python-is-python3 python3-pip -y
         echo 'export PATH=${PATH:+${PATH}:}$HOME/.local/bin/' >> "$HOME"/.bashrc && source "$HOME"/.bashrc
         pip3 install ansible==7.6.0 ruamel_yaml netaddr jmespath==0.9.5
         ## Install kubespray
         cd /tmp || exit
         git clone https://github.com/kubernetes-sigs/kubespray || true
         cd kubespray || exit
         git checkout release-2.23
         # Copy ``inventory/sample`` as ``inventory/mycluster``
         cp -rfp inventory/sample inventory/main
         CONFIG_FILE=inventory/main/hosts.yaml python3 contrib/inventory_builder/inventory.py "${IPS[@]}"
         sed -i 's/metrics_server_enabled: false/metrics_server_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
         sed -i 's/ingress_nginx_enabled: false/ingress_nginx_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
         sed -i 's/cert_manager_enabled: false/cert_manager_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
         sed -i 's/^# kubectl_localhost: false/kubectl_localhost: true/g' inventory/main/group_vars/k8s_cluster/k8s-cluster.yml
         sed -i 's/helm_enabled: false/helm_enabled: true/g' inventory/main/group_vars/k8s_cluster/addons.yml
         ansible-playbook -i inventory/main/hosts.yaml  --become --become-user=root reset.yml -e reset_confirmation=true -e download_run_once=true
         ansible-playbook -i inventory/main/hosts.yaml  --become --become-user=root cluster.yml
         config_path="/vagrant/configs"
         if [ -d $config_path ]; then
           rm -f $config_path/*
        else
           mkdir -p $config_path
        fi
        cp -i /etc/kubernetes/admin.conf $config_path/config
        touch $config_path/join.sh
        chmod +x $config_path/join.sh
        echo $(kubeadm token create --print-join-command) --ignore-preflight-errors=Swap,NumCPU > $config_path/join.sh
        sudo -i -u vagrant bash << EOF
        whoami
        mkdir -p /home/vagrant/.kube
        sudo cp -i $config_path/config /home/vagrant/.kube/
        sudo chown 1000:1000 /home/vagrant/.kube/config
      SHELL
  end

  (1..NUM_WORKER_NODES).each do |i|
    config.vm.define "node0#{i}" do |node|
      node.vm.hostname = "worker-node0#{i}"
      node.vm.network "private_network", ip: IP_NW + "#{IP_START + i}"
      if settings["shared_folders"]
        settings["shared_folders"].each do |shared_folder|
          node.vm.synced_folder shared_folder["host_path"], shared_folder["vm_path"]
        end
      end
      node.vm.provider "virtualbox" do |vb|
        vb.cpus = settings["nodes"]["workers"]["cpu"]
        vb.memory = settings["nodes"]["workers"]["memory"]
        if settings["cluster_name"] and settings["cluster_name"] != ""
          vb.customize ["modifyvm", :id, "--groups", ("/" + settings["cluster_name"])]
        end
      end
    #   node.vm.provision "shell", 
    #     inline: <<-SHELL
    #       config_path="/vagrant/configs"
    #       /bin/bash $config_path/join.sh -v
    #       sudo -i -u vagrant bash << EOF
    #       whoami
    #       mkdir -p /home/vagrant/.kube
    #       sudo cp -i $config_path/config /home/vagrant/.kube/
    #       sudo chown 1000:1000 /home/vagrant/.kube/config
    #       NODENAME=$(hostname -s)
    #       kubectl label node $(hostname -s) node-role.kubernetes.io/worker=worker
    #       EOF
    #     SHELL
    end
  end

end
